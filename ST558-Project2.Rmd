---
title: "ST558-Project 2"
output: github_document
params:
  data_channel: "bus"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

TBD: Mark

# Data

```{r}
library(tidyverse)

set.seed(3)

# Import News
news <- read_csv("OnlineNewsPopularity/OnlineNewsPopularity.csv")

# Split by channel
news <- news %>%
  pivot_longer(starts_with("data_channel_is_"), 
               names_prefix = "data_channel_is_",
               names_to = "channel") %>%
  filter(value == 1) %>%
  select(-value) %>%
  split(.$channel) 

# Work on channel supplied by params
newsData <- news[[params$data_channel]]
```

## Splitting the data

```{r}
library(caret)

## Setup new data set with only variables of interest
newsSubFinal <- newsData %>% 
  select(LDA_02, avg_negative_polarity, average_token_length, 
         max_negative_polarity, kw_max_avg, LDA_03, kw_avg_avg, 
         weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, 
         weekday_is_thursday, weekday_is_friday, 
         weekday_is_saturday, weekday_is_sunday, shares)

## Train/Test Split
trainIndex <- createDataPartition(newsSubFinal$shares, p = .70, list = FALSE)
newsTrain <- newsSubFinal[trainIndex, ]
newsTest <- newsSubFinal[-trainIndex, ]
```

# Summarizations

__Means__:  
We want to look at how the means of all of the variables differ across all the reports(data channels). We want to look specifically at the shares and see which data channel has the highest amount and which has the lowest.
```{r}
data.frame(Means = colMeans(newsTest))
```

__Variance__:  
Similarly for the variance we want to see how these values differ across data channels. Which data channels have the smallest amount of variance and which have the least. 
```{r}
data.frame(Variance = sapply(newsTest, var))
```
__Contigencies__:  
Below we should analyze how the counts for each publishing day differed across data channels.  
In the Boxplots below we can see how the number of shares differed for the different  
publishing days.
```{r}
#Monday
contTabMonday <- table(Monday = newsTrain$weekday_is_monday)

#Tuesday
contTabTuesday <- table(Tuesday = newsTrain$weekday_is_tuesday)

#Wednesday
contTabWednesday <- table(Wednesday = newsTrain$weekday_is_wednesday)
#Thursday
contTabThursday <- table(Thursday = newsTrain$weekday_is_thursday)

#Friday
contTabFriday <- table(Friday = newsTrain$weekday_is_friday)

#Saturday
contTabSaturday <- table(Saturday = newsTrain$weekday_is_saturday)

#Sunday
contTabSunday <- table(Sunday = newsTrain$weekday_is_sunday)

## Collect into dataframe
data.frame(Monday = contTabMonday[2], Tuesday = contTabTuesday[2], Wednesday = contTabWednesday[2], Thursday = contTabThursday[2], Friday = contTabFriday[2], Saturday = contTabSaturday[2], Sunday = contTabSunday[2])
```
__Correlation Plots__ for Numeric variables used:  
Let's analyze the plots below to see if we can identify any positive or negative
relationships between the shares and the predictor.
```{r}
## It seems like very few observations have shares greater than 1500
## Fixing range for better visuals
ggplot(data = newsTrain, aes(shares, LDA_02)) +
  geom_point(color="darkseagreen") +
  labs(x = "Shares", y = "LDA Topic 2 Closeness", title = "LDA2 v. Shares") +
  xlim(0, 15000) 

ggplot(data = newsTrain, aes(shares, avg_negative_polarity)) +
  geom_point(color="darkseagreen") +
  labs(x = "Shares", y = "Average Polarity of Negative Words", 
       title = "Avg. Negative Polarity v. Shares") +
  xlim(0, 15000) 

ggplot(data = newsTrain, aes(shares, average_token_length)) +
  geom_point(color="darkorange") +
  labs(x = "Shares", y = "Avg. Word Length in Content", 
       title = "Avg. Word Length v. Shares") +
  xlim(0, 15000) 

ggplot(data = newsTrain, aes(shares, max_negative_polarity)) +
  geom_point(color="darkorange") +
  labs(x = "Shares", y = "Max. Polarity of Negative Words", 
       title = "Max Negative Polarity v. Shares") +
  xlim(0, 15000) 

ggplot(data = newsTrain, aes(shares, kw_max_avg)) +
  geom_point(color="aquamarine2") +
  labs(x = "Shares", y = "Avg. Keyword/Max. Shares", 
       title = "Avg. Keyword/Max. Shares v. Shares") +
  xlim(0, 15000)

ggplot(data = newsTrain, aes(shares, LDA_03)) +
  geom_point(color="aquamarine2") +
  labs(x = "Shares", y = "LDA Topic 3 Closeness", 
       title = "LDA3 v. Shares") +
  xlim(0, 15000)

ggplot(data = newsTrain, aes(shares, kw_avg_avg)) +
  geom_point(color="darkseagreen") +
  labs(x = "Shares", y = "Avg. Keyword/Avg. Shares", 
       title = "Avg. Keyword/Shares v. Shares") +
  xlim(0, 15000) 
```
  
__Boxplot__(for categorical variable used):

```{r}
# Get days of week in a single column and plot visuals based on weekday for shares
newsTrainDaysSub <- newsTrain %>% select(shares, weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday)

newsTrainDaysSubLong <- pivot_longer(data = newsTrainDaysSub, cols = c(weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday), names_to = "Day")
## Getting observations where the publication for each day is true and not false
newsTrainDaysSubLong <- newsTrainDaysSubLong[newsTrainDaysSubLong$value==1, ]

ggplot(newsTrainDaysSubLong, aes(shares)) +
  geom_boxplot(aes(fill=Day)) +
  scale_fill_discrete(labels = c("Friday", "Monday",  "Saturday","Sunday", "Thursday","Tuesday", "Wednesday")) +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  xlim(0, 15000)
```

Each group member is responsible for producing some summary statistics (means, sds, contingency tables, etc.) and for producing at least three graphs (each) of the data.

# Modeling

The first group member should fit a random forest model and the second group member should
fit a boosted tree model. Both models should be chosen using cross-validation.

Prior to the models fit using linear regression, the first group member should provide a short
but thorough explanation of the idea of a linear regression model.

Prior to each ensemble model, you should provide a short but reasonably thorough explanation
of the ensemble model you are using (so one for each group member).

## Linear Regression
Linear regression is used to model the relationship between a continuous response and one or more explanatory variables. It can be used for both prediction and for inference. Linear regression models are usually fitted by minimizing the least squares residuals(although other methods exist) to estimate the betas. The term linear is in reference to the betas(parameters) not necessarily linear in the predictors. The predictors can have polynomial terms and can also be categorical.  
*  SLR = Regression with only one predictor.
*  MLR = Regression with more than one predictor.
```{r}
fullLM <- train(shares ~ ., data = newsTrain,
                method = "lm",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number= 5))
pred <- predict(fullLM, select(newsTest, -shares))
testFullLM <- postResample(pred, newsTest$shares)
```

```{r}
library(leaps)
fwdFit <- train(shares ~ ., data = newsTrain,
                method = "leapForward",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number= 5),
                verbose = FALSE)
pred <- predict(fwdFit, select(newsTest, -shares))
testFwdFit <- postResample(pred, newsTest$shares)
```

## Random Forest
```{r}
rfFit <- train(shares ~ ., data = newsTrain,
               method = "rf",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "cv", number = 5),
               verbose = FALSE)
pred <- predict(rfFit, select(newsTest, -shares))
testRfFit <- postResample(pred, newsTest$shares)
```


## Boosted Tree

```{r}
gbmFit <- train(shares ~ ., data = newsTrain,
                method = "gbm",
                preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number= 5),
                verbose = FALSE)
pred <- predict(gbmFit, select(newsTest, -shares))
testGbmFit <- postResample(pred, newsTest$shares)
```


# Comparison

Let's compare the methods:
```{r}
(comp <- bind_rows(list(fullLM = testFullLM, 
                        fwdFit = testFwdFit, 
                        rfFit = testRfFit, 
                        gbmFit = testGbmFit), .id = "id"))

slice(comp, which.min(comp$RMSE))
```


# Automation

Below is the manual part that has to be done in order to kick off the  
automated reports
Let's create the file names and params for the render code.

```{r eval = FALSE}
## Create file names
data_channels <- names(news)
output_file <- paste0(data_channels, ".html")

## Create list for each data channel with just the channel name param.
params <- lapply(data_channels, FUN = function(x){list(data_channel = x)})

## Put into df
reports <- tibble(output_file, params)

## Render code
apply(reports, MARGIN = 1,
      FUN = function(x){
        rmarkdown::render(input = "ST558-Project2.Rmd",
               output_format = "github_document",
               output_file = x[[1]],
               params = x[[2]])
      })
```
